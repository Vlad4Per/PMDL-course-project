{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 84792,
     "databundleVersionId": 9524838,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.442655Z",
     "iopub.execute_input": "2024-09-12T22:52:03.443692Z",
     "iopub.status.idle": "2024-09-12T22:52:03.458976Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.443645Z",
     "shell.execute_reply": "2024-09-12T22:52:03.457699Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:33:41.511991Z",
     "start_time": "2024-10-06T12:33:41.507836Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": "## 1.1 Data preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# necessary imports\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport torchvision.transforms as transforms\nfrom torchvision.io import read_image",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.460416Z",
     "iopub.execute_input": "2024-09-12T22:52:03.460769Z",
     "iopub.status.idle": "2024-09-12T22:52:03.472832Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.460734Z",
     "shell.execute_reply": "2024-09-12T22:52:03.471690Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:33:41.530106Z",
     "start_time": "2024-10-06T12:33:41.525998Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def load_img(fname):\n",
    "    \"\"\"\n",
    "    Load an image from file, do transformation (including possible augmentation) and return it as torch.tensor\n",
    "\n",
    "    :param fname: path to jpg image\n",
    "    \"\"\"\n",
    "    img = read_image(fname)\n",
    "    x = img / 255.\n",
    "\n",
    "    # Write your code here\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((200, 200)),                # Resize image to 224x224 (or the size of your model input)\n",
    "            transforms.RandomHorizontalFlip(p=0.5),       # Random horizontal flip with 50% probability\n",
    "            transforms.RandomRotation(degrees=15),        # Randomly rotate the image by up to 15 degrees\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "            v2.ToDtype(torch.float32, scale=True),  # Convert to float and scale\n",
    "            # Keep color augmentations minimal as we're detecting hair color:\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.474469Z",
     "iopub.execute_input": "2024-09-12T22:52:03.474841Z",
     "iopub.status.idle": "2024-09-12T22:52:03.491139Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.474795Z",
     "shell.execute_reply": "2024-09-12T22:52:03.490064Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:33:41.550696Z",
     "start_time": "2024-10-06T12:33:41.545422Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T12:33:41.594434Z",
     "start_time": "2024-10-06T12:33:41.563932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import walk\n",
    "\n",
    "decode = {\n",
    "    0: \"1-5\",1: \"5-10\",2: \"10-15\",3: \"15-20\",4: \"20-25\",5: \"25-30\",6: \"30-35\",7: \"35-40\",8: \"40-45\",9: \"45-50\",10: \"50-55\",11: \"55-60\",12: \"60-65\",13: \"65-70\",14: \"70-75\",15: \"75-80\",16: \"80-85\",17: \"85-90\",18: \"90-95\",19: \"95-100\"\n",
    "}\n",
    "encode = {\n",
    "    \"1-5\":0,\"5-10\":1,\"10-15\":2,\"15-20\":3,\"20-25\":4,\"25-30\":5,\"30-35\":6,\"35-40\":7,\"40-45\":8,\"45-50\":9,\"50-55\":10,\"55-60\":11,\"60-65\":12,\"65-70\":13,\"70-75\":14,\"75-80\":15,\"80-85\":16,\"85-90\":17,\"90-95\":18,\"95-100\":19\n",
    "}\n",
    "img_path = \"C:/Users/pv010/OneDrive/Рабочий стол/Homework/PMDL/project/facial-age\"\n",
    "# _id = []\n",
    "# _age = []\n",
    "# dirs_ages = []\n",
    "# for (dirpath, dirnames, filenames) in walk(img_path):\n",
    "#     dirs_ages.extend(dirnames)\n",
    "#     break\n",
    "# for dire in dirs_ages:\n",
    "#     for (dirpath, dirnames, filenames) in walk(img_path + \"/\" + dire):\n",
    "#         _id.extend(list(map(lambda x:dire+\"/\"+x,filenames)))\n",
    "#         _age.extend([to_range[int(dire) // 5] for _ in range(len(filenames))])\n",
    "#         break\n",
    "# \n",
    "# features_data = {\"ID\": _id, \"Age\": _age}\n",
    "\n",
    "features = pd.read_csv(\"data.csv\")\n",
    "features['Age'] = list(map(lambda x:encode[x], features['Age']))\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "#img_path = \"/kaggle/input/pmldl-week-2-dnn-training-with-tracking-tools/archive\"\n",
    "\n",
    "# Image attributes\n",
    "#train_features = pd.read_csv(f\"{img_path}/train.csv\")\n",
    "\n",
    "# Load and transform images \n",
    "images = torch.stack(\n",
    "    [load_img(f\"{img_path}/{item['ID']}\") for _, item in features.iterrows()])\n",
    "\n",
    "# Write your code here\n",
    "# Select label(s) from train_features\n",
    "labels = features.get('Age')\n",
    "# Leave values that only 1 or 0 and convert to float just for simplicity\n",
    "labels = torch.from_numpy(labels.to_numpy()).float()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.493677Z",
     "iopub.execute_input": "2024-09-12T22:52:03.494068Z",
     "iopub.status.idle": "2024-09-12T22:54:19.124245Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.494029Z",
     "shell.execute_reply": "2024-09-12T22:54:19.122976Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:34:12.754145Z",
     "start_time": "2024-10-06T12:33:41.607540Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": "# Just some checking of shapes\nimages.shape, labels.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:19.125711Z",
     "iopub.execute_input": "2024-09-12T22:54:19.126073Z",
     "iopub.status.idle": "2024-09-12T22:54:19.133275Z",
     "shell.execute_reply.started": "2024-09-12T22:54:19.126037Z",
     "shell.execute_reply": "2024-09-12T22:54:19.131823Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:34:13.952685Z",
     "start_time": "2024-10-06T12:34:13.809158Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9773, 3, 224, 224]), torch.Size([9773]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "source": "## 1.3 Data loaders creation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "processed_dataset = TensorDataset(images, labels)\n",
    "\n",
    "# Write your code here\n",
    "# Set proportion and split dataset into train and validation parts\n",
    "proportion = 0.9\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    processed_dataset,\n",
    "    [int(len(images) * 0.7)+1, int(len(images) * 0.15)+1, int(len(images) * 0.15)],\n",
    ")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:19.624878Z",
     "iopub.execute_input": "2024-09-12T22:54:19.625288Z",
     "iopub.status.idle": "2024-09-12T22:54:19.632823Z",
     "shell.execute_reply.started": "2024-09-12T22:54:19.625244Z",
     "shell.execute_reply": "2024-09-12T22:54:19.631485Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:34:14.076947Z",
     "start_time": "2024-10-06T12:34:14.004304Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "# Create Dataloaders for training and validation \n# Dataloader is iterable object over dataset\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:19.634680Z",
     "iopub.execute_input": "2024-09-12T22:54:19.635046Z",
     "iopub.status.idle": "2024-09-12T22:54:20.031783Z",
     "shell.execute_reply.started": "2024-09-12T22:54:19.635009Z",
     "shell.execute_reply": "2024-09-12T22:54:20.030633Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T12:34:14.155732Z",
     "start_time": "2024-10-06T12:34:14.107980Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Training\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class CNNClassificationModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     MLP (multi-layer perceptron) based classification model for MNIST\n",
    "#     \"\"\"\n",
    "# \n",
    "#     def __init__(self, num_classes=20):\n",
    "#         super(CNNClassificationModel, self).__init__()\n",
    "# \n",
    "#         # Add fully connected layers to nn.Sequential to create MLP\n",
    "#         # First layer should take 28x28 vector\n",
    "#         # last layer should return vector of size num_classes\n",
    "#         # do not forget to add activation function between layers\n",
    "# \n",
    "#         self.block1 = nn.Sequential(\n",
    "#             nn.BatchNorm2d(3),\n",
    "#             nn.Conv2d(3, 4, kernel_size=(3, 3), stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(4),\n",
    "#         )\n",
    "# \n",
    "#         self.block2 = nn.Sequential(\n",
    "#             nn.Conv2d(4, 8, kernel_size=(3, 3)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(8),\n",
    "#         )\n",
    "# \n",
    "#         self.out = nn.Sequential(\n",
    "#             nn.Linear(95048, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = self.block1(x)\n",
    "#         x = self.block2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.out(x)\n",
    "#         return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:58:12.262937Z",
     "iopub.execute_input": "2024-09-12T22:58:12.263487Z",
     "iopub.status.idle": "2024-09-12T22:58:12.274631Z",
     "shell.execute_reply.started": "2024-09-12T22:58:12.263441Z",
     "shell.execute_reply": "2024-09-12T22:58:12.273197Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T13:25:24.132673Z",
     "start_time": "2024-10-06T13:25:24.115416Z"
    }
   },
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T13:44:11.316787Z",
     "start_time": "2024-10-06T13:44:11.304040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class CNNClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based classification model for age prediction with 20 classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=20):  # Changed to 20 classes\n",
    "        super(CNNClassificationModel, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(3),  # assuming 3-channel input (RGB image)\n",
    "            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # reduces spatial size by half\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        # Flatten and fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(6272, 512),  # assuming 128 filters with 3x3 spatial size after conv layers\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)  # final output layer with 20 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)  # Apply first block\n",
    "        x = self.block2(x)  # Apply second block\n",
    "        x = self.block3(x)  # Apply third block\n",
    "        x = self.block4(x)  # Apply fourth block\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for fully connected layers\n",
    "        x = self.fc(x)  # Apply the fully connected layers\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "source": [
    "def train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        writer=None,\n",
    "        epochs=1,\n",
    "        device=\"cpu\",\n",
    "        ckpt_path=\"best.pt\",\n",
    "):\n",
    "    # best score for checkpointing\n",
    "    best = 0.0\n",
    "\n",
    "    # iterating over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # training loop description\n",
    "        train_loop = tqdm(\n",
    "            enumerate(train_loader, 0), total=len(train_loader), desc=f\"Epoch {epoch}\"\n",
    "        )\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        # iterate over dataset \n",
    "        for data in train_loop:\n",
    "            # Write your code here\n",
    "            # Move data to a device, do forward pass and loss calculation, do backward pass and run optimizer\n",
    "            id, (inputs, labels) = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.type(torch.int64)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loop.set_postfix({\"loss\": loss.item()})\n",
    "        # write loss to tensorboard\n",
    "        if writer:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss / len(train_loader), epoch)\n",
    "\n",
    "        # Validation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()  # evaluation mode\n",
    "            val_loop = tqdm(enumerate(val_loader, 0), total=len(val_loader), desc=\"Val\")\n",
    "            for data in val_loop:\n",
    "                id, (inputs, labels) = data\n",
    "\n",
    "                # Write your code here\n",
    "                # Get predictions and compare them with labels\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.type(torch.int64)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                for i, j in zip(predicted, labels):\n",
    "                    if i == j: correct += 1\n",
    "\n",
    "                val_loop.set_postfix({\"acc\": correct / total})\n",
    "\n",
    "            if correct / total > best:\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                best = correct / total\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:20.046643Z",
     "iopub.execute_input": "2024-09-12T22:54:20.047140Z",
     "iopub.status.idle": "2024-09-12T22:54:20.060442Z",
     "shell.execute_reply.started": "2024-09-12T22:54:20.047086Z",
     "shell.execute_reply": "2024-09-12T22:54:20.059190Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T13:37:46.591559Z",
     "start_time": "2024-10-06T13:37:46.581497Z"
    }
   },
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3 Combining everything together",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Write your code here\n",
    "# Pick optimizer from torch.optim and loss function loss_fn from torch.nn that suits best the model\n",
    "# SummaryWriter is used by tensorboard and could be set None\n",
    "model = CNNClassificationModel()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device='cpu',\n",
    "    writer=SummaryWriter(),\n",
    "    epochs=3\n",
    ")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:58:17.164571Z",
     "iopub.execute_input": "2024-09-12T22:58:17.165023Z",
     "iopub.status.idle": "2024-09-12T23:07:28.870641Z",
     "shell.execute_reply.started": "2024-09-12T22:58:17.164981Z",
     "shell.execute_reply": "2024-09-12T23:07:28.869202Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T13:45:43.822536Z",
     "start_time": "2024-10-06T13:44:15.683016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 0:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ada99e76adb64f7c92449c0d7efd2da5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac1680958b9247d3bb4b24c66d20f33d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cbda329b564d47fc9d022990d3d2249a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1b042498cb2430593fad70fa48979c0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9390d8e18b6747c284ae3fe6e52a3f0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ba902a00bb64539ad3a8829da46dae5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 109
  },
  {
   "cell_type": "markdown",
   "source": "## 2.4 Inference\nHere you need to perform inference of trained model on test data. \n\nLoad the best checkpoint from training to the model and run inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# load best checkpoint to model\nmodel = CNNClassificationModel()\nckpt = torch.load(\"best.pt\")\nmodel.load_state_dict(ckpt)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T23:09:50.760106Z",
     "iopub.execute_input": "2024-09-12T23:09:50.760621Z",
     "iopub.status.idle": "2024-09-12T23:09:51.279987Z",
     "shell.execute_reply.started": "2024-09-12T23:09:50.760577Z",
     "shell.execute_reply": "2024-09-12T23:09:51.278837Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Run model inference on test data\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # evaluation mode\n",
    "        test_loop = tqdm(enumerate(test_loader, 0), total=len(test_loader), desc=\"Test\")\n",
    "\n",
    "        for inputs in test_loop:\n",
    "            # Write your code here\n",
    "            # Similar to validation part in training cell\n",
    "            id, pred = inputs\n",
    "            pred = pred.to(device)\n",
    "            _, predicted = torch.max(model(pred).data, 1)\n",
    "\n",
    "            # Extend overall predictions by prediction for a batch\n",
    "            predictions.extend([i.item() for i in predicted])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_training_img(fname):\n",
    "    img = read_image(fname)\n",
    "    x = img / 255.\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            # Write your code here\n",
    "            # Do not apply data augmentation as in training function, but make sure the size of input image is the same\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T23:09:53.088663Z",
     "iopub.execute_input": "2024-09-12T23:09:53.089148Z",
     "iopub.status.idle": "2024-09-12T23:09:53.098352Z",
     "shell.execute_reply.started": "2024-09-12T23:09:53.089102Z",
     "shell.execute_reply": "2024-09-12T23:09:53.097085Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# process test data and run inference on it\n",
    "test_features = pd.read_csv(f\"{img_path}/test.csv\")\n",
    "images = torch.stack(\n",
    "    [load_img(f\"{img_path}/img_align_celeba/test/{item['image_id']}\") for _, item in test_features.iterrows()])\n",
    "\n",
    "test_loader = DataLoader(images, batch_size=batch_size, shuffle=False)\n",
    "predictions = predict(model, test_loader, device='cpu')\n",
    "\n",
    "# generate the submission file\n",
    "submission_df = pd.DataFrame(columns=['ID', 'Blond_Hair'])\n",
    "submission_df['ID'] = test_features.index\n",
    "submission_df['Blond_Hair'] = predictions\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T23:10:41.451022Z",
     "iopub.execute_input": "2024-09-12T23:10:41.451486Z",
     "iopub.status.idle": "2024-09-12T23:10:50.886004Z",
     "shell.execute_reply.started": "2024-09-12T23:10:41.451446Z",
     "shell.execute_reply": "2024-09-12T23:10:50.884850Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
