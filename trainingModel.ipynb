{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 84792,
     "databundleVersionId": 9524838,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30761,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "from pyexpat import features\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.442655Z",
     "iopub.execute_input": "2024-09-12T22:52:03.443692Z",
     "iopub.status.idle": "2024-09-12T22:52:03.458976Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.443645Z",
     "shell.execute_reply": "2024-09-12T22:52:03.457699Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:44:34.241763Z",
     "start_time": "2024-10-06T18:44:34.237560Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "## 1.1 Data preprocessing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# necessary imports\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom tqdm.notebook import tqdm\n\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport torchvision.transforms as transforms\nfrom torchvision.io import read_image",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.460416Z",
     "iopub.execute_input": "2024-09-12T22:52:03.460769Z",
     "iopub.status.idle": "2024-09-12T22:52:03.472832Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.460734Z",
     "shell.execute_reply": "2024-09-12T22:52:03.471690Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:44:39.696137Z",
     "start_time": "2024-10-06T18:44:34.244964Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def load_img(fname):\n",
    "    \"\"\"\n",
    "    Load an image from file, do transformation (including possible augmentation) and return it as torch.tensor\n",
    "\n",
    "    :param fname: path to jpg image\n",
    "    \"\"\"\n",
    "    img = read_image(fname)\n",
    "    x = img / 255.\n",
    "\n",
    "    # Write your code here\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((200, 200)),                        # Resize image to 224x224\n",
    "            transforms.RandomHorizontalFlip(p=0.5),               # Horizontal flip with 50% probability\n",
    "            transforms.ColorJitter(brightness=0.1,                # Minimal brightness change\n",
    "                                   contrast=0.1,                  # Minimal contrast change\n",
    "                                   saturation=0.1,                # Minimal saturation change\n",
    "                                   hue=0.05),                     # Minimal hue shift\n",
    "            transforms.RandomRotation(degrees=5),                 # Small rotation to avoid distortions\n",
    "            transforms.RandomGrayscale(p=1),                    # Convert to grayscale with 10% probability\n",
    "            transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)), # Slight blur for robustness\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            v2.ToDtype(torch.float32, scale=True),  # Convert to float and scale\n",
    "            # Keep color augmentations minimal as we're detecting hair color:\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return transform(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.474469Z",
     "iopub.execute_input": "2024-09-12T22:52:03.474841Z",
     "iopub.status.idle": "2024-09-12T22:52:03.491139Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.474795Z",
     "shell.execute_reply": "2024-09-12T22:52:03.490064Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:44:39.874747Z",
     "start_time": "2024-10-06T18:44:39.840052Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:44:39.902191Z",
     "start_time": "2024-10-06T18:44:39.880355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from os import walk\n",
    "\n",
    "decode = {\n",
    "    0: \"1-5\",1: \"5-10\",2: \"10-15\",3: \"15-20\",4: \"20-25\",5: \"25-30\",6: \"30-40\", 7: \"40-50\", 8: \"50-60\", 9: \"60-70\", 10: \"70+\"\n",
    "}\n",
    "encode = {\n",
    "    \"1-5\":0,\"5-10\":1,\"10-15\":2,\"15-20\":3,\"20-25\":4,\"25-30\":5,\"30-40\":6,\"40-50\":7,\"50-60\":8,\"60-70\":9,\"70+\":10\n",
    "}\n",
    "img_path = \"facial-age\"\n",
    "# _id = []\n",
    "# _age = []\n",
    "# dirs_ages = []\n",
    "# for (dirpath, dirnames, filenames) in walk(img_path):\n",
    "#     dirs_ages.extend(dirnames)\n",
    "#     break\n",
    "# for dire in dirs_ages:\n",
    "#     for (dirpath, dirnames, filenames) in walk(img_path + \"/\" + dire):\n",
    "#         _id.extend(list(map(lambda x:dire+\"/\"+x,filenames)))\n",
    "#         if 1<=int(dire)<5:\n",
    "#             _age.extend([\"1-5\" for _ in range(len(filenames))])\n",
    "#         elif 5<=int(dire)<10:\n",
    "#             _age.extend([\"5-10\" for _ in range(len(filenames))])\n",
    "#         elif 10<=int(dire)<15:\n",
    "#             _age.extend([\"10-15\" for _ in range(len(filenames))])\n",
    "#         elif 15<=int(dire)<20:\n",
    "#             _age.extend([\"15-20\" for _ in range(len(filenames))])\n",
    "#         elif 20<=int(dire)<25:\n",
    "#             _age.extend([\"20-25\" for _ in range(len(filenames))])\n",
    "#         elif 25<=int(dire)<30:\n",
    "#             _age.extend([\"25-30\" for _ in range(len(filenames))])\n",
    "#         elif 30<=int(dire)<40:\n",
    "#             _age.extend([\"30-40\" for _ in range(len(filenames))])\n",
    "#         elif 40<=int(dire)<50:\n",
    "#             _age.extend([\"40-50\" for _ in range(len(filenames))])\n",
    "#         elif 50<=int(dire)<60:\n",
    "#             _age.extend([\"50-60\" for _ in range(len(filenames))])\n",
    "#         elif 60<=int(dire)<70:\n",
    "#             _age.extend([\"60-70\" for _ in range(len(filenames))])\n",
    "#         elif 70<=int(dire):\n",
    "#             _age.extend([\"70+\" for _ in range(len(filenames))])\n",
    "#         break\n",
    "# \n",
    "# features_data = {\"ID\": _id, \"Age\": _age}\n",
    "# features = pd.DataFrame(features_data)\n",
    "# features.to_csv(\"data.csv\", index=False)\n",
    "features = pd.read_csv(\"data.csv\")\n",
    "features['Age'] = list(map(lambda x:encode[x], features['Age']))\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:44:39.914480Z",
     "start_time": "2024-10-06T18:44:39.912066Z"
    }
   },
   "cell_type": "code",
   "source": "#facial-age",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "#img_path = \"/kaggle/input/pmldl-week-2-dnn-training-with-tracking-tools/archive\"\n",
    "\n",
    "# Image attributes\n",
    "#train_features = pd.read_csv(f\"{img_path}/train.csv\")\n",
    "\n",
    "# Load and transform images \n",
    "images = torch.stack(\n",
    "    [load_img(f\"{img_path}/{item['ID']}\") for _, item in features.iterrows()])\n",
    "\n",
    "# Write your code here\n",
    "# Select label(s) from train_features\n",
    "labels = features.get('Age')\n",
    "# Leave values that only 1 or 0 and convert to float just for simplicity\n",
    "labels = torch.from_numpy(labels.to_numpy()).float()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:52:03.493677Z",
     "iopub.execute_input": "2024-09-12T22:52:03.494068Z",
     "iopub.status.idle": "2024-09-12T22:54:19.124245Z",
     "shell.execute_reply.started": "2024-09-12T22:52:03.494029Z",
     "shell.execute_reply": "2024-09-12T22:54:19.122976Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.299898Z",
     "start_time": "2024-10-06T18:44:39.922441Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Just some checking of shapes\nimages.shape, labels.shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:19.125711Z",
     "iopub.execute_input": "2024-09-12T22:54:19.126073Z",
     "iopub.status.idle": "2024-09-12T22:54:19.133275Z",
     "shell.execute_reply.started": "2024-09-12T22:54:19.126037Z",
     "shell.execute_reply": "2024-09-12T22:54:19.131823Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.421959Z",
     "start_time": "2024-10-06T18:46:07.411404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9773, 3, 200, 200]), torch.Size([9773]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "## 1.3 Data loaders creation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "processed_dataset = TensorDataset(images, labels)\n",
    "\n",
    "# Write your code here\n",
    "# Set proportion and split dataset into train and validation parts\n",
    "proportion = 0.9\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    processed_dataset,\n",
    "    [int(len(images) * 0.7)+1, int(len(images) * 0.15)+1, int(len(images) * 0.15)],\n",
    ")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:19.624878Z",
     "iopub.execute_input": "2024-09-12T22:54:19.625288Z",
     "iopub.status.idle": "2024-09-12T22:54:19.632823Z",
     "shell.execute_reply.started": "2024-09-12T22:54:19.625244Z",
     "shell.execute_reply": "2024-09-12T22:54:19.631485Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.499042Z",
     "start_time": "2024-10-06T18:46:07.489071Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "# Create Dataloaders for training and validation \n# Dataloader is iterable object over dataset\nbatch_size = 64\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:19.634680Z",
     "iopub.execute_input": "2024-09-12T22:54:19.635046Z",
     "iopub.status.idle": "2024-09-12T22:54:20.031783Z",
     "shell.execute_reply.started": "2024-09-12T22:54:19.635009Z",
     "shell.execute_reply": "2024-09-12T22:54:20.030633Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.578323Z",
     "start_time": "2024-10-06T18:46:07.570686Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "## 2. Training\n",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class CNNClassificationModel(nn.Module):\n",
    "#     \"\"\"\n",
    "#     MLP (multi-layer perceptron) based classification model for MNIST\n",
    "#     \"\"\"\n",
    "# \n",
    "#     def __init__(self, num_classes=20):\n",
    "#         super(CNNClassificationModel, self).__init__()\n",
    "# \n",
    "#         # Add fully connected layers to nn.Sequential to create MLP\n",
    "#         # First layer should take 28x28 vector\n",
    "#         # last layer should return vector of size num_classes\n",
    "#         # do not forget to add activation function between layers\n",
    "# \n",
    "#         self.block1 = nn.Sequential(\n",
    "#             nn.BatchNorm2d(3),\n",
    "#             nn.Conv2d(3, 4, kernel_size=(3, 3), stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(4),\n",
    "#         )\n",
    "# \n",
    "#         self.block2 = nn.Sequential(\n",
    "#             nn.Conv2d(4, 8, kernel_size=(3, 3)),\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm2d(8),\n",
    "#         )\n",
    "# \n",
    "#         self.out = nn.Sequential(\n",
    "#             nn.Linear(95048, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(1024, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.4),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         x = self.block1(x)\n",
    "#         x = self.block2(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.out(x)\n",
    "#         return x\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:58:12.262937Z",
     "iopub.execute_input": "2024-09-12T22:58:12.263487Z",
     "iopub.status.idle": "2024-09-12T22:58:12.274631Z",
     "shell.execute_reply.started": "2024-09-12T22:58:12.263441Z",
     "shell.execute_reply": "2024-09-12T22:58:12.273197Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.603453Z",
     "start_time": "2024-10-06T18:46:07.597058Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.634645Z",
     "start_time": "2024-10-06T18:46:07.625323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class CNNClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN-based classification model for age prediction with 20 classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=11):  # Changed to 20 classes\n",
    "        super(CNNClassificationModel, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(3),  # assuming 3-channel input (RGB image)\n",
    "            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # reduces spatial size by half\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        # Flatten and fully connected layers for classification\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4608, 512),  # assuming 128 filters with 3x3 spatial size after conv layers\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128, num_classes)  # final output layer with 20 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)  # Apply first block\n",
    "        x = self.block2(x)  # Apply second block\n",
    "        x = self.block3(x)  # Apply third block\n",
    "        x = self.block4(x)  # Apply fourth block\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the output for fully connected layers\n",
    "        x = self.fc(x)  # Apply the fully connected layers\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def train(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        writer=None,\n",
    "        epochs=1,\n",
    "        device=\"cpu\",\n",
    "        ckpt_path=\"best.pt\",\n",
    "):\n",
    "    # best score for checkpointing\n",
    "    best = 0.0\n",
    "\n",
    "    # iterating over epochs\n",
    "    for epoch in range(epochs):\n",
    "        # training loop description\n",
    "        train_loop = tqdm(\n",
    "            enumerate(train_loader, 0), total=len(train_loader), desc=f\"Epoch {epoch}\"\n",
    "        )\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        # iterate over dataset \n",
    "        for data in train_loop:\n",
    "            # Write your code here\n",
    "            # Move data to a device, do forward pass and loss calculation, do backward pass and run optimizer\n",
    "            id, (inputs, labels) = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.type(torch.int64)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_loop.set_postfix({\"loss\": loss.item()})\n",
    "        # write loss to tensorboard\n",
    "        if writer:\n",
    "            writer.add_scalar(\"Loss/train\", train_loss / len(train_loader), epoch)\n",
    "\n",
    "        # Validation\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()  # evaluation mode\n",
    "            val_loop = tqdm(enumerate(val_loader, 0), total=len(val_loader), desc=\"Val\")\n",
    "            for data in val_loop:\n",
    "                id, (inputs, labels) = data\n",
    "\n",
    "                # Write your code here\n",
    "                # Get predictions and compare them with labels\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.type(torch.int64)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                for i, j in zip(predicted, labels):\n",
    "                    if i == j: correct += 1\n",
    "\n",
    "                val_loop.set_postfix({\"acc\": correct / total})\n",
    "\n",
    "            if correct / total > best:\n",
    "                torch.save(model.state_dict(), ckpt_path)\n",
    "                best = correct / total\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:54:20.046643Z",
     "iopub.execute_input": "2024-09-12T22:54:20.047140Z",
     "iopub.status.idle": "2024-09-12T22:54:20.060442Z",
     "shell.execute_reply.started": "2024-09-12T22:54:20.047086Z",
     "shell.execute_reply": "2024-09-12T22:54:20.059190Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:46:07.689519Z",
     "start_time": "2024-10-06T18:46:07.681799Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3 Combining everything together",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Write your code here\n",
    "# Pick optimizer from torch.optim and loss function loss_fn from torch.nn that suits best the model\n",
    "# SummaryWriter is used by tensorboard and could be set None\n",
    "model = CNNClassificationModel()\n",
    "\n",
    "train(\n",
    "    model=model,\n",
    "    optimizer=optim.Adam(model.parameters(), lr=0.001),\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device='cpu',\n",
    "    writer=SummaryWriter(),\n",
    "    epochs=8\n",
    ")\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T22:58:17.164571Z",
     "iopub.execute_input": "2024-09-12T22:58:17.165023Z",
     "iopub.status.idle": "2024-09-12T23:07:28.870641Z",
     "shell.execute_reply.started": "2024-09-12T22:58:17.164981Z",
     "shell.execute_reply": "2024-09-12T23:07:28.869202Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T19:03:33.663888Z",
     "start_time": "2024-10-06T18:59:37.890276Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Epoch 0:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "638581e8b7d34e398313c77f9d0762da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91933840d3bf40e596a3d577d15268aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f0dc8f03ee8c4d76ac3c0cf5beff74af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d55efbf4be0d4909b2adbf2e8b92e13a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "59b2b643cab6410a83af0f9c8fc26fd5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4895af05bf054964b322d3be598d1213"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7c411207aa9048238a5969755bf9b085"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29ea395607724aacb9e16daa8bfad27a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3f9dec4a55043d58898aac8815c411a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17c2159bf10f4b629bbc221dcd75b0a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba904a2b62234eb897976d0d1f9e38e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a40be84b23ab400fa9a2398b340e3f0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f99f3f59c50347d7915b74b177b2e3b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2adccb3fa3844dc5aaadb2555941c15d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7:   0%|          | 0/107 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9bf8f6361b79451eb31b72a630d995e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e60d66f80fc54ffe993f9adf3251644c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": "## 2.4 Inference\nHere you need to perform inference of trained model on test data. \n\nLoad the best checkpoint from training to the model and run inference",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# load best checkpoint to model\nmodel = CNNClassificationModel()\nckpt = torch.load(\"best.pt\")\nmodel.load_state_dict(ckpt)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T23:09:50.760106Z",
     "iopub.execute_input": "2024-09-12T23:09:50.760621Z",
     "iopub.status.idle": "2024-09-12T23:09:51.279987Z",
     "shell.execute_reply.started": "2024-09-12T23:09:50.760577Z",
     "shell.execute_reply": "2024-09-12T23:09:51.278837Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:51:11.459188Z",
     "start_time": "2024-10-06T18:51:11.393867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "def predict(model, test_loader, device):\n",
    "    \"\"\"\n",
    "    Run model inference on test data\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()  # evaluation mode\n",
    "        test_loop = tqdm(enumerate(test_loader, 0), total=len(test_loader), desc=\"Test\")\n",
    "\n",
    "        for inputs in test_loop:\n",
    "            # Write your code here\n",
    "            # Similar to validation part in training cell\n",
    "            id, pred = inputs\n",
    "            pred = pred.to(device)\n",
    "            _, predicted = torch.max(model(pred).data, 1)\n",
    "\n",
    "            # Extend overall predictions by prediction for a batch\n",
    "            predictions.extend([i.item() for i in predicted])\n",
    "        return predictions\n",
    "\n",
    "\n",
    "def load_training_img(fname):\n",
    "    img = read_image(fname)\n",
    "    x = img / 255.\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            # Write your code here\n",
    "            # Do not apply data augmentation as in training function, but make sure the size of input image is the same\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform(x)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T23:09:53.088663Z",
     "iopub.execute_input": "2024-09-12T23:09:53.089148Z",
     "iopub.status.idle": "2024-09-12T23:09:53.098352Z",
     "shell.execute_reply.started": "2024-09-12T23:09:53.089102Z",
     "shell.execute_reply": "2024-09-12T23:09:53.097085Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:51:11.558306Z",
     "start_time": "2024-10-06T18:51:11.551882Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "# process test data and run inference on it\n",
    "test_features = pd.read_csv(f\"{img_path}/test.csv\")\n",
    "images = torch.stack(\n",
    "    [load_img(f\"{img_path}/img_align_celeba/test/{item['image_id']}\") for _, item in test_features.iterrows()])\n",
    "\n",
    "test_loader = DataLoader(images, batch_size=batch_size, shuffle=False)\n",
    "predictions = predict(model, test_loader, device='cpu')\n",
    "\n",
    "# generate the submission file\n",
    "submission_df = pd.DataFrame(columns=['ID', 'Blond_Hair'])\n",
    "submission_df['ID'] = test_features.index\n",
    "submission_df['Blond_Hair'] = predictions\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.head()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-12T23:10:41.451022Z",
     "iopub.execute_input": "2024-09-12T23:10:41.451486Z",
     "iopub.status.idle": "2024-09-12T23:10:50.886004Z",
     "shell.execute_reply.started": "2024-09-12T23:10:41.451446Z",
     "shell.execute_reply": "2024-09-12T23:10:50.884850Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-10-06T18:51:12.068031Z",
     "start_time": "2024-10-06T18:51:11.562311Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'facial-age/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# process test data and run inference on it\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m test_features \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mimg_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/test.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m images \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(\n\u001B[0;32m      4\u001B[0m     [load_img(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/img_align_celeba/test/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m _, item \u001B[38;5;129;01min\u001B[39;00m test_features\u001B[38;5;241m.\u001B[39miterrows()])\n\u001B[0;32m      6\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(images, batch_size\u001B[38;5;241m=\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    617\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    619\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 620\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1617\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1619\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1620\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1878\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1879\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1880\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1881\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1882\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1883\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1884\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1885\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1886\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1887\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1888\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1889\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1890\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1891\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    868\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    871\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    872\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    874\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    875\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    876\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    877\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    878\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    879\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'facial-age/test.csv'"
     ]
    }
   ],
   "execution_count": 16
  }
 ]
}
